{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Progressive Growing GAN is an extension to the GAN training procedure that involves training a GAN to generate very small images, such as 4×4, and incrementally increasing the size of the generated images to 8×8, 16×16, until the desired output size is met. This has allowed the progressive GAN to generate photorealistic synthetic faces with 1024×1024 pixel resolution.\n",
    "\n",
    "Progressive Growing GAN involves using a generator and discriminator model with the same general structure and starting with very small images. During training, new blocks of convolutional layers are systematically added to both the generator model and the discriminator models.\n",
    "\n",
    "The incremental addition of the layers allows the models to effectively learn coarse-level detail and later learn ever-finer detail, both on the generator and discriminator sides.\n",
    "\n",
    "This incremental nature allows the training to first discover large-scale structure of the image distribution and then shift attention to increasingly finer-scale detail, instead of having to learn all scales simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the Celebrity Faces Dataset\n",
    "#### The dataset provides about 200,000 photographs of celebrity faces along with annotations for what appears in given photos, such as glasses, face shape, hats, hair type, etc. As part of the dataset, the authors provide a version of each photo centered on the face and cropped to the portrait with varying sizes around 150 pixels wide and 200 pixels tall. We will use this as the basis for developing our GAN model.  \n",
    "#### https://www.kaggle.com/jessicali9530/celeba-dataset   (img_align_celeba.zip)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Further, as we are only interested in the face in each photo and not the background, we can perform face detection and extract only the face before resizing the result to a fixed size.\n",
    "we will use a pre-trained Multi-Task Cascaded Convolutional Neural Network, or MTCNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.9\n"
     ]
    }
   ],
   "source": [
    "# sudo pip install mtcnn\n",
    "import mtcnn\n",
    "\n",
    "print(mtcnn.__version__)\n",
    "# # prepare the model\n",
    "# model = MTCNN()\n",
    "# # detect faces in an image\n",
    "# faces = model.detect_faces(pixels)\n",
    "# # extract details of the face\n",
    "# x1, y1, width, height = faces[0]['box']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "reduce the size of the generated images to 128×128 which will, in turn, allow us to train a reasonable model on a GPU in a few hours and still discover how the progressive growing model can be implemented, trained, and used.\n",
    "As such, we can develop a function to load a file and extract the face from the photo, then and resize the extracted face pixels to a predefined size. In this case, we will use the square shape of 128×128 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image as an rgb numpy array\n",
    "def load_image(filename):\n",
    "    \n",
    "    # load an image from file\n",
    "    image = Image.open(filename)\n",
    "    # convert to rgb \n",
    "    image = image.convert('RGB')\n",
    "    # convert to array\n",
    "    pixels = asarray(image)\n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MTCNN model and pixel values for a single photograph as arguments and returns a 128x128x3 array of pixel values with just the face, or None if no face was detected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## follow the following link to change the mode on UBUNTU for overcommit handling (its a huge dataset)\n",
    "#### https://stackoverflow.com/questions/57507832/unable-to-allocate-array-with-shape-and-data-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the face from a loaded image and resize\n",
    "def extract_face(model, pixels, required_size=(128, 128)):\n",
    "    \n",
    "    #detect face in an image\n",
    "    faces = model.detect_faces(pixels)\n",
    "    # skip cases where we could not detect a face\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    \n",
    "    # extract face details\n",
    "    x1, y1, width, height = faces[0]['box']\n",
    "    \n",
    "    # force detected pixel to be positive\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    # convert into coordinates\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    \n",
    "    # retrieve face pixels\n",
    "    face_pixels = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face_pixels)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    \n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "enumerate all photograph files in a directory and extracts and resizes the face from each and returns a NumPy array of faces. \n",
    "limit the total number of faces loaded via the n_faces argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory, n_faces):\n",
    "    \n",
    "    # prepare the model\n",
    "    model = MTCNN()\n",
    "    faces = list()\n",
    "    \n",
    "    # enumarate the files\n",
    "    for filename in listdir(directory):\n",
    "        \n",
    "        # load the image\n",
    "        pixels = load_image(directory + filename)\n",
    "        # get face\n",
    "        face = extract_face(model, pixels)\n",
    "        if face is None:\n",
    "            continue\n",
    "        \n",
    "        # store\n",
    "        faces.append(face)\n",
    "        print(len(faces), face.shape)\n",
    "        \n",
    "        # stop once we have enough\n",
    "        if len(faces) >= n_faces:\n",
    "            break\n",
    "            \n",
    "        return asarray(faces)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "complete example of preparing a dataset of celebrity faces for training a GAN model is listed below.\n",
    "increase the total number of loaded faces to 50,000 to provide a good training dataset for our GAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_faces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ebe9eec13baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/img_align_celeba/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# load and extract all faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mall_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_faces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# save in compressed format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_faces' is not defined"
     ]
    }
   ],
   "source": [
    "# example of extracting and resizing faces into a new dataset\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# directory containing all images\n",
    "directory = 'data/img_align_celeba/'\n",
    "# load and extract all faces\n",
    "all_faces = load_faces(directory, 50000)\n",
    "print(\"Loaded: \", all_faces.shape)\n",
    "# save in compressed format\n",
    "savez_compressed('data/processed/img_align_caleba_128.npz', all_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/rex/Desktop/py resources/GAN/Progressive Growing GAN/data/processed/img_align_celeba_128.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0d3f042768e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load the face dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/rex/Desktop/py resources/GAN/Progressive Growing GAN/data/processed/img_align_celeba_128.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/rex/Desktop/py resources/GAN/Progressive Growing GAN/data/processed/img_align_celeba_128.npz'"
     ]
    }
   ],
   "source": [
    "# load the prepared dataset\n",
    "from numpy import load\n",
    "# load the face dataset\n",
    "data = load('/home/rex/Desktop/py resources/GAN/Progressive Growing GAN/data/processed/img_align_celeba_128.npz')\n",
    "faces = data['arr_0']\n",
    "\n",
    "print(\"Loaded: \", faces.shape)\n",
    "faces[1:1500]\n",
    "pyplot.imshow(faces[0].astype('uint8'))\n",
    "pyplot.show(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " elaborate on this example and plot the first 100 faces in the dataset as a 10×10 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "\n",
    "# plot a list of loaded faces\n",
    "def plot_faces(faces, n):\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        pyplot.subplot(n, n, i+1)\n",
    "        # turn off axis\n",
    "        pyplot.axis('off')\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(faces[i].astype('uint8'))\n",
    "    pyplot.show()\n",
    "\n",
    "# load the face dataset\n",
    "data = load('data/processed/img_align_celeba_128.npz')\n",
    "faces = data['arr_0']\n",
    "\n",
    "print(\"Loaded: \", faces.shape)\n",
    "plot_faces(faces, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-2f1e5290d5a2>, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-2f1e5290d5a2>\"\u001b[0;36m, line \u001b[0;32m57\u001b[0m\n\u001b[0;31m    if len(faces) >= n_faces:\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# example of extracting and resizing faces into a new dataset\n",
    "from os import listdir\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from PIL import Image\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# load an image as an rgb numpy array\n",
    "def load_image(filename):\n",
    "\t# load image from file\n",
    "\timage = Image.open(filename)\n",
    "\t# convert to RGB, if needed\n",
    "\timage = image.convert('RGB')\n",
    "\t# convert to array\n",
    "\tpixels = asarray(image)\n",
    "\treturn pixels\n",
    " \n",
    "# extract the face from a loaded image and resize\n",
    "def extract_face(model, pixels, required_size=(128, 128)):\n",
    "\t# detect face in the image\n",
    "\tfaces = model.detect_faces(pixels)\n",
    "\t# skip cases where we could not detect a face\n",
    "\tif len(faces) == 0:\n",
    "\t\treturn None\n",
    "\t# extract details of the face\n",
    "\tx1, y1, width, height = faces[0]['box']\n",
    "\t# force detected pixel values to be positive (bug fix)\n",
    "\tx1, y1 = abs(x1), abs(y1)\n",
    "\t# convert into coordinates\n",
    "\tx2, y2 = x1 + width, y1 + height\n",
    "\t# retrieve face pixels\n",
    "\tface_pixels = pixels[y1:y2, x1:x2]\n",
    "\t# resize pixels to the model size\n",
    "\timage = Image.fromarray(face_pixels)\n",
    "\timage = image.resize(required_size)\n",
    "\tface_array = asarray(image)\n",
    "\treturn face_array\n",
    " \n",
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory, n_faces):\n",
    "\t# prepare model\n",
    "\tmodel = MTCNN()\n",
    "\tfaces = list()\n",
    "\t# enumerate files\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# load the image\n",
    "\t\tpixels = load_image(directory + filename)\n",
    "\t\t# get face\n",
    "\t\tface = extract_face(model, pixels)\n",
    "\t\tif face is None:\n",
    "\t\t\tcontinue\n",
    "\t\t# store\n",
    "\t\tfaces.append(face)\n",
    "\t\tprint(len(faces), face.shape)\n",
    "\t\t# stop once we have enough\n",
    "\t\tif len(faces) >= n_faces:\n",
    "\t\t\tbreak\n",
    "\treturn asarray(faces)\n",
    " \n",
    "# directory that contains all images\n",
    "directory = 'data/img_align_celeba/'\n",
    "# load and extract all faces\n",
    "all_faces = load_faces(directory, 1500)\n",
    "print('Loaded: ', all_faces.shape)\n",
    "# save in compressed format\n",
    "savez_compressed('data/processed/img_align_celeba_128.npz', all_faces)\n",
    "\n",
    "from numpy import load\n",
    "# load the face dataset\n",
    "data = load('data/processed/img_align_celeba_128.npz')\n",
    "faces = data['arr_0']\n",
    "print('Loaded: ', faces.shape)\n",
    "\n",
    "# load the prepared dataset\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# plot a list of loaded faces\n",
    "def plot_faces(faces, n):\n",
    "\tfor i in range(n * n):\n",
    "\t\t# define subplot\n",
    "\t\tpyplot.subplot(n, n, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tpyplot.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tpyplot.imshow(faces[i].astype('uint8'))\n",
    "\tpyplot.show()\n",
    " \n",
    "# load the face dataset \n",
    "data = load('/home/rex/Desktop/py resources/GAN/Progressive Growing GAN/data/processed/img_align_celeba_128.npz')\n",
    "faces = data['arr_0']\n",
    "print('Loaded: ', faces.shape)\n",
    "plot_faces(faces, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Develop Progressive Growing GAN Models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Has 3 custom layers:\n",
    "1. weightedSum - controls weighted sum of the old and new layers during the growth phase.\n",
    "2. MinibatchStdev - summarizes statistics for a batch of images in the discriminator.\n",
    "3. PixelNormalization - normalizes activation maps in the generator model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WeightedSum Layer\n",
    "merge layer that combines activations from 2 input layers, such as 2 input types in discriminator or 2 output paths in the generator model.\n",
    "Used during the growth phase of training when model transitions from one image size to a new image size with double the width and height.\n",
    "Growth phase alpha is scaled from 0.0 to 1.0 allowing output of the layer to transition from giving full weight to the new layers/2nd input\n",
    "\n",
    "weighted sum = ((1.0 - alpha) * input1) + (alpha * input2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# weighted sum output\n",
    "class WeightedSum(Add):\n",
    "    \n",
    "    # init with default value\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "        self.alpha = backend.variable(alpha, name='ws_alpha')\n",
    "        \n",
    "    # output a weighted sum of inputs\n",
    "    def _merge_function(self, inputs):\n",
    "        \n",
    "        # only support a weighted sum of two inputs\n",
    "        assert(len(inputs) == 2)\n",
    "        \n",
    "        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MinibatchStdev\n",
    "output block of the discriminator layer \n",
    "objective - provide statistical summary of the batch of operations.\n",
    "the discriminator can then learn to detect batches of fake samples from batches of real samples encouraging the generator to generate batches of samples with realistic batch statistics\n",
    "the standard deviation for each pixel value in the activation maps across the batch, calculating the average of this value, and then creating a new activation map (one channel) that is appended to the list of activation maps provided as input."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PixelNormalization\n",
    "generator and discriminator dont use batch normalization, each pixel in the activation maps is normalized to unit length, i.e pixelwise feature vector normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Growing GAN Discriminator Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cnn that expects a 4x4 color image as input and predicts whether its real or fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loss function as the average predicted value multiplied by the target value.\n",
    "The target value will be 1 for real images and -1 for fake images. \n",
    "This means that weight updates will seek to increase the divide between \n",
    "real and fake images.\n",
    "'''\n",
    "# wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "discriminator is called by specifying the number of blocks to create\n",
    "function returns a list where each element contains two models\n",
    "1. normal model/straight through \n",
    "2. model version including old 1x1 and new block with the weighted sum, used for transition/growth phase training\n",
    "create 6 blocks creating 6 pairs of models that expect the input image sizes of 4x4, 8x8,...,128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a discriminator block\n",
    "def add_discriminator_block(old_model, n_input_layers=3):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = max_norm(1.0)\n",
    "\t# get shape of existing model\n",
    "\tin_shape = list(old_model.input.shape)\n",
    "\t# define new input shape as double the size\n",
    "\tinput_shape = (in_shape[-2].value*2, in_shape[-2].value*2, in_shape[-1].value)\n",
    "\tin_image = Input(shape=input_shape)\n",
    "\t# define new input processing layer\n",
    "\td = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# define new block\n",
    "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\td = AveragePooling2D()(d)\n",
    "\tblock_new = d\n",
    "\t# skip the input, 1x1 and activation for the old model\n",
    "\tfor i in range(n_input_layers, len(old_model.layers)):\n",
    "\t\td = old_model.layers[i](d)\n",
    "\t# define straight-through model\n",
    "\tmodel1 = Model(in_image, d)\n",
    "\t# compile model\n",
    "\tmodel1.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t# downsample the new larger image\n",
    "\tdownsample = AveragePooling2D()(in_image)\n",
    "\t# connect old input processing to downsampled new input\n",
    "\tblock_old = old_model.layers[1](downsample)\n",
    "\tblock_old = old_model.layers[2](block_old)\n",
    "\t# fade in output of old model input layer with new input\n",
    "\td = WeightedSum()([block_old, block_new])\n",
    "\t# skip the input, 1x1 and activation for the old model\n",
    "\tfor i in range(n_input_layers, len(old_model.layers)):\n",
    "\t\td = old_model.layers[i](d)\n",
    "\t# define straight-through model\n",
    "\tmodel2 = Model(in_image, d)\n",
    "\t# compile model\n",
    "\tmodel2.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\treturn [model1, model2]\n",
    "\n",
    "# define the discriminator models for each image resolution\n",
    "def define_discriminator(n_blocks, input_shape=(4,4,3)):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = max_norm(1.0)\n",
    "\tmodel_list = list()\n",
    "\t# base model input\n",
    "\tin_image = Input(shape=input_shape)\n",
    "\t# conv 1x1\n",
    "\td = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# conv 3x3 (output block)\n",
    "\td = MinibatchStdev()(d)\n",
    "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# conv 4x4\n",
    "\td = Conv2D(128, (4,4), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# dense output layer\n",
    "\td = Flatten()(d)\n",
    "\tout_class = Dense(1)(d)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_class)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t# store model\n",
    "\tmodel_list.append([model, model])\n",
    "\t# create submodels\n",
    "\tfor i in range(1, n_blocks):\n",
    "\t\t# get prior model without the fade-on\n",
    "\t\told_model = model_list[i - 1][0]\n",
    "\t\t# create new model for next resolution\n",
    "\t\tmodels = add_discriminator_block(old_model)\n",
    "\t\t# store model\n",
    "\t\tmodel_list.append(models)\n",
    "\treturn model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressive Growing Generator Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Takes a random point from the latent space as input and generates a synthetic image.\n",
    "Described same way as discriminator difference is the growth phase output is output of the WeightedSum Layer.\n",
    "Growth phase involves adding a nearest neighbor unsampling layer connected to the new block with the new output layer and to the old output layer.\n",
    "Old and new output layers are combined via a WeightedSum output layer.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Base model has an input block defn with a fully connected(FC) layer to create a given number of 4x4 feature maps.\n",
    "Followed by 4x4 and 3x3 convolution layers and a 1x1 output layer generating color images.\n",
    "new blocks are added with an unsample layer and two 3x3 conv layers."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LeakyReLU is used and PixelNormalization layer used after each con layer.\n",
    "Linear activation used in output layer.\n",
    "Number of feature maps decrease with depth of model from 512 to 16. Fix all layers to have same number of filters for simplicity in WeightedSum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a generator block\n",
    "def add_generator_block(old_model):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = max_norm(1.0)\n",
    "\t# get the end of the last block\n",
    "\tblock_end = old_model.layers[-2].output\n",
    "\t# upsample, and define new block\n",
    "\tupsampling = UpSampling2D()(block_end)\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(upsampling)\n",
    "\tg = PixelNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\tg = PixelNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# add new output layer\n",
    "\tout_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\t# define model\n",
    "\tmodel1 = Model(old_model.input, out_image)\n",
    "\t# get the output layer from old model\n",
    "\tout_old = old_model.layers[-1]\n",
    "\t# connect the upsampling to the old output layer\n",
    "\tout_image2 = out_old(upsampling)\n",
    "\t# define new output image as the weighted sum of the old and new models\n",
    "\tmerged = WeightedSum()([out_image2, out_image])\n",
    "\t# define model\n",
    "\tmodel2 = Model(old_model.input, merged)\n",
    "\treturn [model1, model2]\n",
    "\n",
    "# define generator models\n",
    "def define_generator(latent_dim, n_blocks, in_dim=4):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = max_norm(1.0)\n",
    "\tmodel_list = list()\n",
    "\t# base model latent input\n",
    "\tin_latent = Input(shape=(latent_dim,))\n",
    "\t# linear scale up to activation maps\n",
    "\tg  = Dense(128 * in_dim * in_dim, kernel_initializer=init, kernel_constraint=const)(in_latent)\n",
    "\tg = Reshape((in_dim, in_dim, 128))(g)\n",
    "\t# conv 4x4, input block\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\tg = PixelNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# conv 3x3\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\tg = PixelNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# conv 1x1, output block\n",
    "\tout_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_latent, out_image)\n",
    "\t# store model\n",
    "\tmodel_list.append([model, model])\n",
    "\t# create submodels\n",
    "\tfor i in range(1, n_blocks):\n",
    "\t\t# get prior model without the fade-on\n",
    "\t\told_model = model_list[i - 1][0]\n",
    "\t\t# create new model for next resolution\n",
    "\t\tmodels = add_generator_block(old_model)\n",
    "\t\t# store model\n",
    "\t\tmodel_list.append(models)\n",
    "\treturn model_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The function returns a list of models where each item in the list contains the normal or straight-through version of each generator and the growth version for phasing in the new block at the larger output image size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite Models for Training the Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator models are not compiled as they are not trained directly  but via discriminator models using Wasserstein loss.\n",
    "i.e presenting generated images as real images and calculating the loss used to update the generator."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Generator mus be paired with discriminator in terms of same image size and phase of training such as growth phase (introducing the new block) or fine-tuning phase(normal or straight-through)\n",
    "Achieved by creating a new model for each pair of models that stacks the generator on top of the discriminator so that the synthetic image feeds directly into the discriminator model to be deemed real or fake.\n",
    "This composite model can be used to train the generator via the discriminator and the weights of the discriminator cn be marked as not trainable ensuring they are not changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs of composite models are created, 6 pairs for the 6 levels of image growth where each pair is comprised of a composite model for the normal model and growth version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composite models for training generator via discriminator\n",
    "def define_composite(discriminators, generators):\n",
    "    \n",
    "    # create composite models\n",
    "    for i in range(len(discriminators)):\n",
    "        g_models, d_models = generators[i], discriminators[i]\n",
    "        \n",
    "        # straight-through/normal model\n",
    "        d_models[0].trainable = False\n",
    "        model1 = Sequential()\n",
    "        model1.add(g_models[0])\n",
    "        model1.add(g_models[0])\n",
    "        model1.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "        \n",
    "        #fade-in model\n",
    "        d_models[1].trainable = False\n",
    "        model2 = Sequential()\n",
    "        model2.add(g_models[1])\n",
    "        model2.add(g_models[1])\n",
    "        model2.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "        \n",
    "        # store\n",
    "        model_list.append([model1, model2])\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Train Progressive Growing GAN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load prepared dataset convert the pixels to floating point values and scale them to the range[-1,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def load_real_samples(filename):\n",
    "    \n",
    "    data = load(filename)\n",
    "    # extract numpy array\n",
    "    X = data['arr_0']\n",
    "    # convert from int to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0, 255] to [-1, 1]\n",
    "    X = (X - 127.5)/127.5\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "retrieve random samples of images used to update discriminator\n",
    "function returns a random sample of images from the loaded dataset and their corresponding target value of class = 1 to indicate the images are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    \n",
    "    # choose random instance\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select images\n",
    "    X = dataset(ix)\n",
    "    # generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "function below takes generator model generates and returns a batch of synthetic images and corresponding target for the discriminator of class = -1 ie fake images.\n",
    "latent points function creates required batch worth of random latent points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# create class labels\n",
    "\ty = -ones((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "function returns a batch of latent points with required dimensionality, i.e sample of latent points used to create synthetic images with the generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    \n",
    "    # gen points in latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    \n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "function below updates the alpha value of WeightedSum layers in the d and g model at a given level which requires a linear transitionfrom 0.0 to 1.0 based on the training step.\n",
    "\n",
    "Given a list of models it locates the WeightedSum in each and sets the value for alpha based on current training step number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use Keras Callback instead\n",
    "\n",
    "# update alpha value on each instance of the weighted sum\n",
    "def update_fadein(models, step, n_steps):\n",
    "    \n",
    "    # calc current alpha\n",
    "    alpha = step / float(n_steps - 1)\n",
    "    \n",
    "    # update the alpha for each model\n",
    "    for model in models:\n",
    "        for layer in model.layers:\n",
    "            if instance(layer, WeightedSum):\n",
    "                if instance(layer, WeightedSum):\n",
    "                    backend.set_value(layer.alpha, alpha)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Function trains d and g models for a single training phase.\n",
    "1st select a half batch of real images from the data and gen 1/2 batch of fake images from the current state of the generator model.\n",
    "G mpdel is updated via d with composite model, indicating the images are real and updating generator weights in an effort to better fool the discriminator\n",
    "\n",
    "suuummary of performmance is printed at the end of each training iteration summarizing loss of the discriminator on real(d1) and fake(d2) images and the loss of the generator(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train generator and discriminator\n",
    "def train_epochs(g_model, d_model, gan_model, dataset, n_epochs, n_batch, fadein=False):\n",
    "\t\n",
    "    # calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# calculate the size of half a batch of samples\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t\n",
    "    # manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# update alpha for all WeightedSum layers when fading in new blocks\n",
    "\t\tif fadein:\n",
    "\t\t\tupdate_fadein([g_model, d_model, gan_model], i, n_steps)\n",
    "\t\t# prepare real and fake samples\n",
    "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator model\n",
    "\t\td_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "\t\td_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tz_input = generate_latent_points(latent_dim, n_batch)\n",
    "\t\ty_real2 = ones((n_batch, 1))\n",
    "\t\tg_loss = gan_model.train_on_batch(z_input, y_real2)\n",
    "\t\t# summarize loss on this batch\n",
    "\t\tprint('>%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, d_loss1, d_loss2, g_loss))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "need to call the train epoch for each training phase, involves scaling the training data to required pixel dimensions.\n",
    "function below returns a scaled dataset instead of rescaling on each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale images to required size\n",
    "def scale_dataset(images, new_shape):\n",
    "    images_list = list()\n",
    "    \n",
    "    for image in images:\n",
    "        # resize with nearest neighbor interpolation\n",
    "        new_image = resize(image, new_shape, 0)\n",
    "        # store\n",
    "        images_list.append(new_image)\n",
    "    \n",
    "    return asarray(images_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "function given a status string i.e 'faded'/'tuned', a g model and latent space size creates a unique name for system state and create a plot of 25 generated images, save the plot a g model to file using defined name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen samples save a plot and model\n",
    "def summarize_performance(status, g_model, latent_dim, n_samples=25):\n",
    "\t# devise name\n",
    "\tgen_shape = g_model.output_shape\n",
    "\tname = '%03dx%03d-%s' % (gen_shape[1], gen_shape[2], status)\n",
    "\t# generate images\n",
    "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# normalize pixel values to the range [0,1]\n",
    "\tX = (X - X.min()) / (X.max() - X.min())\n",
    "\t# plot real images\n",
    "\tsquare = int(sqrt(n_samples))\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(square, square, 1 + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X[i])\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'plot_%s.png' % (name)\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'model_%s.h5' % (name)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train takes list of defined models, a list of batch sizes and number of training epochs for the normal and fade in phases at each level of growth for the model\n",
    "\n",
    "growth steps:\n",
    "scaling image dataset to preferred size,\n",
    "training and saving the fade_in model for the new image size\n",
    "training and saving the normal or fine-tuned model for the new image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_models, d_models, gan_models, dataset, latent_dim, e_norm, e_fadein, n_batch):\n",
    "\t# fit the baseline model\n",
    "\tg_normal, d_normal, gan_normal = g_models[0][0], d_models[0][0], gan_models[0][0]\n",
    "\t# scale dataset to appropriate size\n",
    "\tgen_shape = g_normal.output_shape\n",
    "\tscaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "\tprint('Scaled Data', scaled_data.shape)\n",
    "\t# train normal or straight-through models\n",
    "\ttrain_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm[0], n_batch[0])\n",
    "\tsummarize_performance('tuned', g_normal, latent_dim)\n",
    "\t# process each level of growth\n",
    "\tfor i in range(1, len(g_models)):\n",
    "\t\t# retrieve models for this level of growth\n",
    "\t\t[g_normal, g_fadein] = g_models[i]\n",
    "\t\t[d_normal, d_fadein] = d_models[i]\n",
    "\t\t[gan_normal, gan_fadein] = gan_models[i]\n",
    "\t\t# scale dataset to appropriate size\n",
    "\t\tgen_shape = g_normal.output_shape\n",
    "\t\tscaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "\t\tprint('Scaled Data', scaled_data.shape)\n",
    "\t\t# train fade-in models for next level of growth\n",
    "\t\ttrain_epochs(g_fadein, d_fadein, gan_fadein, scaled_data, e_fadein[i], n_batch[i], True)\n",
    "\t\tsummarize_performance('faded', g_normal, latent_dim)\n",
    "\t\t# train normal or straight-through models\n",
    "\t\ttrain_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm[i], n_batch[i])\n",
    "\t\tsummarize_performance('tuned', g_normal, latent_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Instead of keeping the batch size and number of epochs constant, we vary it to speed up the training process, using larger batch sizes for early training phases and smaller batch sizes for later training phases for fine-tuning and stability. Additionally, fewer training epochs are used for the smaller models and more epochs for the larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of growth phases, e.g. 6 == [4, 8, 16, 32, 64, 128]\n",
    "n_blocks = 6\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# define models\n",
    "d_models = define_discriminator(n_blocks)\n",
    "# define models\n",
    "g_models = define_generator(latent_dim, n_blocks)\n",
    "# define composite models\n",
    "gan_models = define_composite(d_models, g_models)\n",
    "# load image data\n",
    "dataset = load_real_samples('img_align_celeba_128.npz')\n",
    "print('Loaded', dataset.shape)\n",
    "# train model\n",
    "n_batch = [16, 16, 16, 8, 4, 4]\n",
    "# 10 epochs == 500K images per training phase\n",
    "n_epochs = [5, 8, 8, 10, 10, 10]\n",
    "train(g_models, d_models, gan_models, dataset, latent_dim, n_epochs, n_epochs, n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete training code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of progressive growing gan on celebrity faces dataset\n",
    "from math import sqrt\n",
    "from numpy import load\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from skimage.transform import resize\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Add\n",
    "from keras.constraints import max_norm\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import backend\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# pixel-wise feature vector normalization layer\n",
    "class PixelNormalization(Layer):\n",
    "\t# initialize the layer\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(PixelNormalization, self).__init__(**kwargs)\n",
    "\n",
    "\t# perform the operation\n",
    "\tdef call(self, inputs):\n",
    "\t\t# calculate square pixel values\n",
    "\t\tvalues = inputs**2.0\n",
    "\t\t# calculate the mean pixel values\n",
    "\t\tmean_values = backend.mean(values, axis=-1, keepdims=True)\n",
    "\t\t# ensure the mean is not zero\n",
    "\t\tmean_values += 1.0e-8\n",
    "\t\t# calculate the sqrt of the mean squared value (L2 norm)\n",
    "\t\tl2 = backend.sqrt(mean_values)\n",
    "\t\t# normalize values by the l2 norm\n",
    "\t\tnormalized = inputs / l2\n",
    "\t\treturn normalized\n",
    "\n",
    "\t# define the output shape of the layer\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\treturn input_shape\n",
    "\n",
    "# mini-batch standard deviation layer\n",
    "class MinibatchStdev(Layer):\n",
    "\t# initialize the layer\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(MinibatchStdev, self).__init__(**kwargs)\n",
    "\n",
    "\t# perform the operation\n",
    "\tdef call(self, inputs):\n",
    "\t\t# calculate the mean value for each pixel across channels\n",
    "\t\tmean = backend.mean(inputs, axis=0, keepdims=True)\n",
    "\t\t# calculate the squared differences between pixel values and mean\n",
    "\t\tsqu_diffs = backend.square(inputs - mean)\n",
    "\t\t# calculate the average of the squared differences (variance)\n",
    "\t\tmean_sq_diff = backend.mean(squ_diffs, axis=0, keepdims=True)\n",
    "\t\t# add a small value to avoid a blow-up when we calculate stdev\n",
    "\t\tmean_sq_diff += 1e-8\n",
    "\t\t# square root of the variance (stdev)\n",
    "\t\tstdev = backend.sqrt(mean_sq_diff)\n",
    "\t\t# calculate the mean standard deviation across each pixel coord\n",
    "\t\tmean_pix = backend.mean(stdev, keepdims=True)\n",
    "\t\t# scale this up to be the size of one input feature map for each sample\n",
    "\t\tshape = backend.shape(inputs)\n",
    "\t\toutput = backend.tile(mean_pix, (shape[0], shape[1], shape[2], 1))\n",
    "\t\t# concatenate with the output\n",
    "\t\tcombined = backend.concatenate([inputs, output], axis=-1)\n",
    "\t\treturn combined\n",
    "\n",
    "\t# define the output shape of the layer\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\t# create a copy of the input shape as a list\n",
    "\t\tinput_shape = list(input_shape)\n",
    "\t\t# add one to the channel dimension (assume channels-last)\n",
    "\t\tinput_shape[-1] += 1\n",
    "\t\t# convert list to a tuple\n",
    "\t\treturn tuple(input_shape)\n",
    "\n",
    "# weighted sum output\n",
    "class WeightedSum(Add):\n",
    "\t# init with default value\n",
    "\tdef __init__(self, alpha=0.0, **kwargs):\n",
    "\t\tsuper(WeightedSum, self).__init__(**kwargs)\n",
    "\t\tself.alpha = backend.variable(alpha, name='ws_alpha')\n",
    "\n",
    "\t# output a weighted sum of inputs\n",
    "\tdef _merge_function(self, inputs):\n",
    "\t\t# only supports a weighted sum of two inputs\n",
    "\t\tassert (len(inputs) == 2)\n",
    "\t\t# ((1-a) * input1) + (a * input2)\n",
    "\t\toutput = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1])\n",
    "\t\treturn output\n",
    "\n",
    "# calculate wasserstein loss\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "\treturn backend.mean(y_true * y_pred)\n",
    "\n",
    "# add a discriminator block\n",
    "def add_discriminator_block(old_model, n_input_layers=3):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = max_norm(1.0)\n",
    "\t# get shape of existing model\n",
    "\tin_shape = list(old_model.input.shape)\n",
    "\t# define new input shape as double the size\n",
    "\tinput_shape = (in_shape[-2].value*2, in_shape[-2].value*2, in_shape[-1].value)\n",
    "\tin_image = Input(shape=input_shape)\n",
    "\t# define new input processing layer\n",
    "\td = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# define new block\n",
    "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\td = AveragePooling2D()(d)\n",
    "\tblock_new = d\n",
    "\t# skip the input, 1x1 and activation for the old model\n",
    "\tfor i in range(n_input_layers, len(old_model.layers)):\n",
    "\t\td = old_model.layers[i](d)\n",
    "\t# define straight-through model\n",
    "\tmodel1 = Model(in_image, d)\n",
    "\t# compile model\n",
    "\tmodel1.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t# downsample the new larger image\n",
    "\tdownsample = AveragePooling2D()(in_image)\n",
    "\t# connect old input processing to downsampled new input\n",
    "\tblock_old = old_model.layers[1](downsample)\n",
    "\tblock_old = old_model.layers[2](block_old)\n",
    "\t# fade in output of old model input layer with new input\n",
    "\td = WeightedSum()([block_old, block_new])\n",
    "\t# skip the input, 1x1 and activation for the old model\n",
    "\tfor i in range(n_input_layers, len(old_model.layers)):\n",
    "\t\td = old_model.layers[i](d)\n",
    "\t# define straight-through model\n",
    "\tmodel2 = Model(in_image, d)\n",
    "\t# compile model\n",
    "\tmodel2.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\treturn [model1, model2]\n",
    "\n",
    "# define the discriminator models for each image resolution\n",
    "def define_discriminator(n_blocks, input_shape=(4,4,3)):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = max_norm(1.0)\n",
    "\tmodel_list = list()\n",
    "\t# base model input\n",
    "\tin_image = Input(shape=input_shape)\n",
    "\t# conv 1x1\n",
    "\td = Conv2D(128, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(in_image)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# conv 3x3 (output block)\n",
    "\td = MinibatchStdev()(d)\n",
    "\td = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# conv 4x4\n",
    "\td = Conv2D(128, (4,4), padding='same', kernel_initializer=init, kernel_constraint=const)(d)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# dense output layer\n",
    "\td = Flatten()(d)\n",
    "\tout_class = Dense(1)(d)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_class)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t# store model\n",
    "\tmodel_list.append([model, model])\n",
    "\t# create submodels\n",
    "\tfor i in range(1, n_blocks):\n",
    "\t\t# get prior model without the fade-on\n",
    "\t\told_model = model_list[i - 1][0]\n",
    "\t\t# create new model for next resolution\n",
    "\t\tmodels = add_discriminator_block(old_model)\n",
    "\t\t# store model\n",
    "\t\tmodel_list.append(models)\n",
    "\treturn model_list\n",
    "\n",
    "# add a generator block\n",
    "def add_generator_block(old_model):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = max_norm(1.0)\n",
    "\t# get the end of the last block\n",
    "\tblock_end = old_model.layers[-2].output\n",
    "\t# upsample, and define new block\n",
    "\tupsampling = UpSampling2D()(block_end)\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(upsampling)\n",
    "\tg = PixelNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\tg = PixelNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# add new output layer\n",
    "\tout_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\t# define model\n",
    "\tmodel1 = Model(old_model.input, out_image)\n",
    "\t# get the output layer from old model\n",
    "\tout_old = old_model.layers[-1]\n",
    "\t# connect the upsampling to the old output layer\n",
    "\tout_image2 = out_old(upsampling)\n",
    "\t# define new output image as the weighted sum of the old and new models\n",
    "\tmerged = WeightedSum()([out_image2, out_image])\n",
    "\t# define model\n",
    "\tmodel2 = Model(old_model.input, merged)\n",
    "\treturn [model1, model2]\n",
    "\n",
    "# define generator models\n",
    "def define_generator(latent_dim, n_blocks, in_dim=4):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# weight constraint\n",
    "\tconst = max_norm(1.0)\n",
    "\tmodel_list = list()\n",
    "\t# base model latent input\n",
    "\tin_latent = Input(shape=(latent_dim,))\n",
    "\t# linear scale up to activation maps\n",
    "\tg  = Dense(128 * in_dim * in_dim, kernel_initializer=init, kernel_constraint=const)(in_latent)\n",
    "\tg = Reshape((in_dim, in_dim, 128))(g)\n",
    "\t# conv 4x4, input block\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\tg = PixelNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# conv 3x3\n",
    "\tg = Conv2D(128, (3,3), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\tg = PixelNormalization()(g)\n",
    "\tg = LeakyReLU(alpha=0.2)(g)\n",
    "\t# conv 1x1, output block\n",
    "\tout_image = Conv2D(3, (1,1), padding='same', kernel_initializer=init, kernel_constraint=const)(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_latent, out_image)\n",
    "\t# store model\n",
    "\tmodel_list.append([model, model])\n",
    "\t# create submodels\n",
    "\tfor i in range(1, n_blocks):\n",
    "\t\t# get prior model without the fade-on\n",
    "\t\told_model = model_list[i - 1][0]\n",
    "\t\t# create new model for next resolution\n",
    "\t\tmodels = add_generator_block(old_model)\n",
    "\t\t# store model\n",
    "\t\tmodel_list.append(models)\n",
    "\treturn model_list\n",
    "\n",
    "# define composite models for training generators via discriminators\n",
    "def define_composite(discriminators, generators):\n",
    "\tmodel_list = list()\n",
    "\t# create composite models\n",
    "\tfor i in range(len(discriminators)):\n",
    "\t\tg_models, d_models = generators[i], discriminators[i]\n",
    "\t\t# straight-through model\n",
    "\t\td_models[0].trainable = False\n",
    "\t\tmodel1 = Sequential()\n",
    "\t\tmodel1.add(g_models[0])\n",
    "\t\tmodel1.add(d_models[0])\n",
    "\t\tmodel1.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t\t# fade-in model\n",
    "\t\td_models[1].trainable = False\n",
    "\t\tmodel2 = Sequential()\n",
    "\t\tmodel2.add(g_models[1])\n",
    "\t\tmodel2.add(d_models[1])\n",
    "\t\tmodel2.compile(loss=wasserstein_loss, optimizer=Adam(lr=0.001, beta_1=0, beta_2=0.99, epsilon=10e-8))\n",
    "\t\t# store\n",
    "\t\tmodel_list.append([model1, model2])\n",
    "\treturn model_list\n",
    "\n",
    "# load dataset\n",
    "def load_real_samples(filename):\n",
    "\t# load dataset\n",
    "\tdata = load(filename)\n",
    "\t# extract numpy array\n",
    "\tX = data['arr_0']\n",
    "\t# convert from ints to floats\n",
    "\tX = X.astype('float32')\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tX = (X - 127.5) / 127.5\n",
    "\treturn X\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# select images\n",
    "\tX = dataset[ix]\n",
    "\t# generate class labels\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# create class labels\n",
    "\ty = -ones((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# update the alpha value on each instance of WeightedSum\n",
    "def update_fadein(models, step, n_steps):\n",
    "\t# calculate current alpha (linear from 0 to 1)\n",
    "\talpha = step / float(n_steps - 1)\n",
    "\t# update the alpha for each model\n",
    "\tfor model in models:\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\tif isinstance(layer, WeightedSum):\n",
    "\t\t\t\tbackend.set_value(layer.alpha, alpha)\n",
    "\n",
    "# train a generator and discriminator\n",
    "def train_epochs(g_model, d_model, gan_model, dataset, n_epochs, n_batch, fadein=False):\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# calculate the size of half a batch of samples\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# update alpha for all WeightedSum layers when fading in new blocks\n",
    "\t\tif fadein:\n",
    "\t\t\tupdate_fadein([g_model, d_model, gan_model], i, n_steps)\n",
    "\t\t# prepare real and fake samples\n",
    "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator model\n",
    "\t\td_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "\t\td_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tz_input = generate_latent_points(latent_dim, n_batch)\n",
    "\t\ty_real2 = ones((n_batch, 1))\n",
    "\t\tg_loss = gan_model.train_on_batch(z_input, y_real2)\n",
    "\t\t# summarize loss on this batch\n",
    "\t\tprint('>%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "\n",
    "# scale images to preferred size\n",
    "def scale_dataset(images, new_shape):\n",
    "\timages_list = list()\n",
    "\tfor image in images:\n",
    "\t\t# resize with nearest neighbor interpolation\n",
    "\t\tnew_image = resize(image, new_shape, 0)\n",
    "\t\t# store\n",
    "\t\timages_list.append(new_image)\n",
    "\treturn asarray(images_list)\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(status, g_model, latent_dim, n_samples=25):\n",
    "\t# devise name\n",
    "\tgen_shape = g_model.output_shape\n",
    "\tname = '%03dx%03d-%s' % (gen_shape[1], gen_shape[2], status)\n",
    "\t# generate images\n",
    "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# normalize pixel values to the range [0,1]\n",
    "\tX = (X - X.min()) / (X.max() - X.min())\n",
    "\t# plot real images\n",
    "\tsquare = int(sqrt(n_samples))\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(square, square, 1 + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X[i])\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'plot_%s.png' % (name)\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'model_%s.h5' % (name)\n",
    "\tg_model.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_models, d_models, gan_models, dataset, latent_dim, e_norm, e_fadein, n_batch):\n",
    "\t# fit the baseline model\n",
    "\tg_normal, d_normal, gan_normal = g_models[0][0], d_models[0][0], gan_models[0][0]\n",
    "\t# scale dataset to appropriate size\n",
    "\tgen_shape = g_normal.output_shape\n",
    "\tscaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "\tprint('Scaled Data', scaled_data.shape)\n",
    "\t# train normal or straight-through models\n",
    "\ttrain_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm[0], n_batch[0])\n",
    "\tsummarize_performance('tuned', g_normal, latent_dim)\n",
    "\t# process each level of growth\n",
    "\tfor i in range(1, len(g_models)):\n",
    "\t\t# retrieve models for this level of growth\n",
    "\t\t[g_normal, g_fadein] = g_models[i]\n",
    "\t\t[d_normal, d_fadein] = d_models[i]\n",
    "\t\t[gan_normal, gan_fadein] = gan_models[i]\n",
    "\t\t# scale dataset to appropriate size\n",
    "\t\tgen_shape = g_normal.output_shape\n",
    "\t\tscaled_data = scale_dataset(dataset, gen_shape[1:])\n",
    "\t\tprint('Scaled Data', scaled_data.shape)\n",
    "\t\t# train fade-in models for next level of growth\n",
    "\t\ttrain_epochs(g_fadein, d_fadein, gan_fadein, scaled_data, e_fadein[i], n_batch[i], True)\n",
    "\t\tsummarize_performance('faded', g_normal, latent_dim)\n",
    "\t\t# train normal or straight-through models\n",
    "\t\ttrain_epochs(g_normal, d_normal, gan_normal, scaled_data, e_norm[i], n_batch[i])\n",
    "\t\tsummarize_performance('tuned', g_normal, latent_dim)\n",
    "\n",
    "# number of growth phases, e.g. 6 == [4, 8, 16, 32, 64, 128]\n",
    "n_blocks = 6\n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# define models\n",
    "d_models = define_discriminator(n_blocks)\n",
    "# define models\n",
    "g_models = define_generator(latent_dim, n_blocks)\n",
    "# define composite models\n",
    "gan_models = define_composite(d_models, g_models)\n",
    "# load image data\n",
    "dataset = load_real_samples('img_align_celeba_128.npz')\n",
    "print('Loaded', dataset.shape)\n",
    "# train model\n",
    "n_batch = [16, 16, 16, 8, 4, 4]\n",
    "# 10 epochs == 500K images per training phase\n",
    "n_epochs = [5, 8, 8, 10, 10, 10]\n",
    "train(g_models, d_models, gan_models, dataset, latent_dim, n_epochs, n_epochs, n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
